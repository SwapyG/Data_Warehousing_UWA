{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded data with 51284 rows and 25 columns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading the excel file \n",
    "print(\"Loading data...\")\n",
    "df = pd.read_excel('Coordinates_Imputed_Final.xlsx')\n",
    "print(f\"Loaded data with {len(df)} rows and {df.columns.shape[0]} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create output directory\n",
    "output_dir = 'dimension_tables'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "# Function to save dataframe to CSV\n",
    "def save_to_csv(df, filename):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved: {file_path} with {len(df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Victim Dimension...\n",
      "Saved: dimension_tables/victim_dimension.csv with 1071 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========= 1. Victim Dimension =========\n",
    "# Extract unique combinations of victim attributes\n",
    "victim_dim = df[['Road User', 'Gender', 'Age', 'Age Group']].drop_duplicates()\n",
    "# Generate surrogate key\n",
    "victim_dim = victim_dim.reset_index(drop=True)\n",
    "victim_dim['victim_key'] = victim_dim.index + 1\n",
    "# Rename columns\n",
    "victim_dim = victim_dim.rename(columns={\n",
    "    'Road User': 'road_user_type',\n",
    "    'Gender': 'gender',\n",
    "    'Age': 'age',\n",
    "    'Age Group': 'age_group'\n",
    "})\n",
    "# Reorder columns\n",
    "victim_dim = victim_dim[['victim_key', 'road_user_type', 'gender', 'age', 'age_group']]\n",
    "save_to_csv(victim_dim, 'victim_dimension.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Crash Event Dimension...\n",
      "Saved: dimension_tables/crash_event_dimension.csv with 51284 rows\n"
     ]
    }
   ],
   "source": [
    "# ========= 2. Crash Event Dimension =========\n",
    "# Extract unique combinations of crash event attributes\n",
    "crash_dim = df[['Crash ID', 'Crash Type', 'Number Fatalities']].drop_duplicates()\n",
    "\n",
    "# Generate sequential surrogate key\n",
    "crash_dim = crash_dim.reset_index(drop=True)\n",
    "crash_dim['crash_id'] = crash_dim.index + 1\n",
    "\n",
    "# Rename columns\n",
    "crash_dim = crash_dim.rename(columns={\n",
    "    'Crash ID': 'crash_event',  # Change from 'Crash Event' to 'Crash ID'\n",
    "    'Crash Type': 'crash_type',\n",
    "    'Number Fatalities': 'number_fatalities'\n",
    "})\n",
    "\n",
    "# Reorder columns\n",
    "crash_dim = crash_dim[['crash_id', 'crash_event', 'crash_type', 'number_fatalities']]\n",
    "save_to_csv(crash_dim, 'crash_event_dimension.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Date Dimension...\n",
      "Saved: dimension_tables/date_dimension.csv with 3024 rows\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. Date Dimension =========\n",
    "# Extract unique combinations of date attributes\n",
    "date_dim = df[['Year', 'Month', 'Dayweek']].drop_duplicates()\n",
    "\n",
    "# Create sequential surrogate key\n",
    "date_dim = date_dim.reset_index(drop=True)\n",
    "date_dim['date_key'] = date_dim.index + 1  # Sequential surrogate key\n",
    "\n",
    "# Add season and quarter\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Summer'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Autumn'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Winter'\n",
    "    else:\n",
    "        return 'Spring'\n",
    "\n",
    "def get_quarter(month):\n",
    "    return (month - 1) // 3 + 1\n",
    "    \n",
    "date_dim['season'] = date_dim['Month'].apply(get_season)\n",
    "date_dim['quarter'] = date_dim['Month'].apply(get_quarter)\n",
    "\n",
    "# Add month name\n",
    "month_names = {\n",
    "    1: 'January', 2: 'February', 3: 'March', 4: 'April', \n",
    "    5: 'May', 6: 'June', 7: 'July', 8: 'August',\n",
    "    9: 'September', 10: 'October', 11: 'November', 12: 'December'\n",
    "}\n",
    "date_dim['month_name'] = date_dim['Month'].map(month_names)\n",
    "\n",
    "# Rename columns\n",
    "date_dim = date_dim.rename(columns={\n",
    "    'Year': 'year',\n",
    "    'Month': 'month',\n",
    "    'Dayweek': 'day_of_week'\n",
    "})\n",
    "\n",
    "# Reorder columns\n",
    "date_dim = date_dim[['date_key', 'year', 'month', 'month_name', 'day_of_week', 'quarter', 'season']]\n",
    "save_to_csv(date_dim, 'date_dimension.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Time of Day Dimension...\n",
      "Saved: dimension_tables/time_of_day_dimension.csv with 1530 rows\n"
     ]
    }
   ],
   "source": [
    "# ========= 4. Time of Day Dimension =========\n",
    "# Extract hour and minute from the Time column\n",
    "df_time = df.copy()\n",
    "df_time['Hour'] = pd.to_datetime(df_time['Time'], format='%H:%M:%S', errors='coerce').dt.hour\n",
    "df_time['Minute'] = pd.to_datetime(df_time['Time'], format='%H:%M:%S', errors='coerce').dt.minute\n",
    "\n",
    "# Extract unique combinations of time attributes\n",
    "time_dim = df_time[['Hour', 'Minute', 'Time of Day']].drop_duplicates()\n",
    "\n",
    "# Create sequential surrogate key\n",
    "time_dim = time_dim.reset_index(drop=True)\n",
    "time_dim['time_key'] = time_dim.index + 1  # Sequential surrogate key\n",
    "\n",
    "# Add original time value for reference (HHMM format)\n",
    "time_dim['time_value'] = time_dim.apply(\n",
    "    lambda row: int(f\"{0 if pd.isna(row['Hour']) else int(row['Hour']):02d}\"\n",
    "                    f\"{0 if pd.isna(row['Minute']) else int(row['Minute']):02d}\"), \n",
    "    axis=1)\n",
    "\n",
    "# Add time category\n",
    "def get_time_category(hour):\n",
    "    if pd.isna(hour):\n",
    "        return 'Unknown'\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "        \n",
    "time_dim['time_category'] = time_dim['Hour'].apply(get_time_category)\n",
    "\n",
    "# Clean up NaN values\n",
    "time_dim = time_dim.fillna({'Hour': -1, 'Minute': -1})\n",
    "\n",
    "# Rename columns\n",
    "time_dim = time_dim.rename(columns={\n",
    "    'Hour': 'hour',\n",
    "    'Minute': 'minute',\n",
    "    'Time of Day': 'daylight_indicator'\n",
    "})\n",
    "\n",
    "# Reorder columns\n",
    "time_dim = time_dim[['time_key', 'hour', 'minute', 'daylight_indicator', 'time_category']]\n",
    "save_to_csv(time_dim, 'time_of_day_dimension.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Location Dimension...\n",
      "Saved: dimension_tables/location_dimension.csv with 585 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========= 5. Location Dimension =========\n",
    "# Extract unique combinations of location attributes\n",
    "location_dim = df[['State', 'SA4 Name 2021', 'National LGA Name 2021']].drop_duplicates()\n",
    "# Generate surrogate key\n",
    "location_dim = location_dim.reset_index(drop=True)\n",
    "location_dim['location_key'] = location_dim.index + 1\n",
    "# Rename columns\n",
    "location_dim = location_dim.rename(columns={\n",
    "    'State': 'state',\n",
    "    'SA4 Name 2021': 'sa4_region',\n",
    "    'National LGA Name 2021': 'lga_name'\n",
    "})\n",
    "# Reorder columns\n",
    "location_dim = location_dim[['location_key', 'state', 'sa4_region', 'lga_name']]\n",
    "save_to_csv(location_dim, 'location_dimension.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Remoteness Dimension...\n",
      "Saved: dimension_tables/remoteness_dimension.csv with 5 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========= 6. Remoteness Dimension =========\n",
    "# Extract unique remoteness areas\n",
    "remoteness_dim = df[['National Remoteness Areas']].drop_duplicates()\n",
    "# Generate surrogate key\n",
    "remoteness_dim = remoteness_dim.reset_index(drop=True)\n",
    "remoteness_dim['remoteness_key'] = remoteness_dim.index + 1\n",
    "# Rename columns\n",
    "remoteness_dim = remoteness_dim.rename(columns={\n",
    "    'National Remoteness Areas': 'remoteness_area'\n",
    "})\n",
    "# Reorder columns\n",
    "remoteness_dim = remoteness_dim[['remoteness_key', 'remoteness_area']]\n",
    "save_to_csv(remoteness_dim, 'remoteness_dimension.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Road Characteristics Dimension...\n",
      "Saved: dimension_tables/road_characteristics_dimension.csv with 95 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========= 7. Road Characteristics Dimension =========\n",
    "# Extract unique combinations of road attributes\n",
    "road_dim = df[['National Road Type', 'Speed Limit']].drop_duplicates()\n",
    "# Add speed category\n",
    "def get_speed_category(speed):\n",
    "    if pd.isna(speed) or speed < 0:\n",
    "        return 'Unknown'\n",
    "    elif speed <= 50:\n",
    "        return 'Low'\n",
    "    elif speed <= 80:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "road_dim['speed_category'] = road_dim['Speed Limit'].apply(get_speed_category)\n",
    "# Generate surrogate key\n",
    "road_dim = road_dim.reset_index(drop=True)\n",
    "road_dim['road_key'] = road_dim.index + 1\n",
    "# Rename columns\n",
    "road_dim = road_dim.rename(columns={\n",
    "    'National Road Type': 'road_type',\n",
    "    'Speed Limit': 'speed_limit'\n",
    "})\n",
    "# Reorder columns\n",
    "road_dim = road_dim[['road_key', 'road_type', 'speed_limit', 'speed_category']]\n",
    "save_to_csv(road_dim, 'road_characteristics_dimension.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Vehicle Involvement Dimension...\n",
      "Saved: dimension_tables/vehicle_involvement_dimension.csv with 7 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========= 8. Vehicle Involvement Dimension =========\n",
    "# Extract unique combinations of vehicle involvement attributes\n",
    "vehicle_dim = df[['Bus Involvement', 'Heavy Rigid Truck Involvement', 'Articulated Truck Involvement']].drop_duplicates()\n",
    "# Add derived attribute\n",
    "vehicle_dim['any_heavy_vehicle'] = vehicle_dim.apply(\n",
    "    lambda row: 'Yes' if row['Bus Involvement'] == 'Yes' or\n",
    "                         row['Heavy Rigid Truck Involvement'] == 'Yes' or\n",
    "                         row['Articulated Truck Involvement'] == 'Yes'\n",
    "                else 'No',\n",
    "    axis=1\n",
    ")\n",
    "# Count vehicle types involved\n",
    "vehicle_dim['vehicle_type_count'] = vehicle_dim.apply(\n",
    "    lambda row: sum([1 for col in ['Bus Involvement', 'Heavy Rigid Truck Involvement', 'Articulated Truck Involvement'] \n",
    "                     if row[col] == 'Yes']),\n",
    "    axis=1\n",
    ")\n",
    "# Generate surrogate key\n",
    "vehicle_dim = vehicle_dim.reset_index(drop=True)\n",
    "vehicle_dim['vehicle_key'] = vehicle_dim.index + 1\n",
    "# Rename columns\n",
    "vehicle_dim = vehicle_dim.rename(columns={\n",
    "    'Bus Involvement': 'bus_involvement',\n",
    "    'Heavy Rigid Truck Involvement': 'heavy_rigid_truck_involvement',\n",
    "    'Articulated Truck Involvement': 'articulated_truck_involvement'\n",
    "})\n",
    "# Reorder columns\n",
    "vehicle_dim = vehicle_dim[['vehicle_key', 'bus_involvement', 'heavy_rigid_truck_involvement', \n",
    "                           'articulated_truck_involvement', 'any_heavy_vehicle', 'vehicle_type_count']]\n",
    "save_to_csv(vehicle_dim, 'vehicle_involvement_dimension.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Fact Table...\n",
      "Saved: dimension_tables/fact_crashes.csv with 51284 rows\n"
     ]
    }
   ],
   "source": [
    "# ========= Create the Fact Table =========\n",
    "# Create lookup dictionaries\n",
    "victim_lookup = dict(zip(\n",
    "    zip(victim_dim['road_user_type'], victim_dim['gender'], victim_dim['age'], victim_dim['age_group']),\n",
    "    victim_dim['victim_key']\n",
    "))\n",
    "\n",
    "crash_lookup = dict(zip(\n",
    "    crash_dim['crash_event'],  \n",
    "    crash_dim['crash_id']  \n",
    "))\n",
    "\n",
    "date_lookup = dict(zip(\n",
    "    zip(date_dim['year'], date_dim['month'], date_dim['day_of_week']),\n",
    "    date_dim['date_key']\n",
    "))\n",
    "\n",
    "# Extract time data\n",
    "df['Hour'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce').dt.hour\n",
    "df['Minute'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce').dt.minute\n",
    "\n",
    "time_lookup = dict(zip(\n",
    "    zip(time_dim['hour'], time_dim['minute']),\n",
    "    time_dim['time_key']\n",
    "))\n",
    "\n",
    "location_lookup = dict(zip(\n",
    "    zip(location_dim['state'], location_dim['sa4_region'], location_dim['lga_name']),\n",
    "    location_dim['location_key']\n",
    "))\n",
    "\n",
    "remoteness_lookup = dict(zip(\n",
    "    remoteness_dim['remoteness_area'],\n",
    "    remoteness_dim['remoteness_key']\n",
    "))\n",
    "\n",
    "road_lookup = dict(zip(\n",
    "    zip(road_dim['road_type'], road_dim['speed_limit']),\n",
    "    road_dim['road_key']\n",
    "))\n",
    "\n",
    "vehicle_lookup = dict(zip(\n",
    "    zip(vehicle_dim['bus_involvement'], vehicle_dim['heavy_rigid_truck_involvement'], \n",
    "        vehicle_dim['articulated_truck_involvement']),\n",
    "    vehicle_dim['vehicle_key']\n",
    "))\n",
    "\n",
    "# Create fact table\n",
    "fact_table = pd.DataFrame()\n",
    "fact_table['crash_event'] = df['Crash ID']  # Use original Crash ID as crash_event\n",
    "fact_table['fact_id'] = range(1, len(df) + 1)  # Unique fact table ID\n",
    "\n",
    "# Add foreign keys\n",
    "fact_table['victim_key'] = df.apply(\n",
    "    lambda row: victim_lookup.get((row['Road User'], row['Gender'], row['Age'], row['Age Group'])), axis=1\n",
    ")\n",
    "\n",
    "fact_table['crash_id'] = df['Crash ID'].map(crash_lookup)\n",
    "\n",
    "fact_table['date_key'] = df.apply(\n",
    "    lambda row: date_lookup.get((row['Year'], row['Month'], row['Dayweek'])), axis=1\n",
    ")\n",
    "\n",
    "fact_table['time_key'] = df.apply(\n",
    "    lambda row: time_lookup.get((row['Hour'], row['Minute'])), axis=1\n",
    ")\n",
    "\n",
    "fact_table['location_key'] = df.apply(\n",
    "    lambda row: location_lookup.get((row['State'], row['SA4 Name 2021'], row['National LGA Name 2021'])), axis=1\n",
    ")\n",
    "\n",
    "fact_table['remoteness_key'] = df['National Remoteness Areas'].map(remoteness_lookup)\n",
    "\n",
    "fact_table['road_key'] = df.apply(\n",
    "    lambda row: road_lookup.get((row['National Road Type'], row['Speed Limit'])), axis=1\n",
    ")\n",
    "\n",
    "fact_table['vehicle_key'] = df.apply(\n",
    "    lambda row: vehicle_lookup.get((row['Bus Involvement'], \n",
    "                                  row['Heavy Rigid Truck Involvement'], \n",
    "                                  row['Articulated Truck Involvement'])), axis=1\n",
    ")\n",
    "\n",
    "# Add basic measures\n",
    "fact_table['fatalities'] = df['Number Fatalities']  # Original fatality count\n",
    "\n",
    "# Add enhanced numerical measures for crash-level analysis\n",
    "\n",
    "\n",
    "# 2. Normalized Fatality Rate - fatalities per 100,000 dwellings\n",
    "fact_table['fatality_rate_per_100k_dwellings'] = df.apply(\n",
    "    lambda row: round((row['Number Fatalities'] / row['Dwelling_Count']) * 100000, 4) if row['Dwelling_Count'] > 0 else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 3. Risk Factor Score - composite score based on speed, time and road factors\n",
    "# Calculate time factor (night is higher risk)\n",
    "time_factor = df['Time of Day'].apply(lambda x: 1.5 if x == 'Night' else 1.0)\n",
    "\n",
    "# Calculate speed factor (higher speeds = higher risk)\n",
    "def get_speed_factor(speed):\n",
    "    if pd.isna(speed):\n",
    "        return 1.0\n",
    "    elif speed <= 50:\n",
    "        return 1.0\n",
    "    elif speed <= 80:\n",
    "        return 1.5\n",
    "    else:\n",
    "        return 2.0\n",
    "\n",
    "speed_factor = df['Speed Limit'].apply(get_speed_factor)\n",
    "\n",
    "# Calculate road type factor\n",
    "road_type_factors = {\n",
    "    'National or State Highway': 1.6,\n",
    "    'Arterial Road': 1.4,\n",
    "    'Sub-arterial Road': 1.3,\n",
    "    'Local Road': 1.0,\n",
    "    'Collector Road': 1.2,\n",
    "    'Access road': 0.9,\n",
    "    'Undetermined': 1.0,\n",
    "    'Pedestrian Thoroughfare': 0.8,\n",
    "    'Busway': 0.7\n",
    "}\n",
    "road_factor = df['National Road Type'].map(road_type_factors).fillna(1.0)\n",
    "\n",
    "# Calculate weekend factor\n",
    "weekend_factor = df['Day of week'].apply(lambda x: 1.2 if x == 'Weekend' else 1.0)\n",
    "\n",
    "# Combined risk factor score\n",
    "fact_table['risk_factor_score'] = round(time_factor * speed_factor * road_factor * weekend_factor,4)\n",
    "\n",
    "\n",
    "# Save fact table\n",
    "save_to_csv(fact_table, 'fact_crashes.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
